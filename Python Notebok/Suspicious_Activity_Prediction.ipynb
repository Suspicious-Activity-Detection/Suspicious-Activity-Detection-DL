{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing the necessary libraries**","metadata":{"_uuid":"b99a65ca-750e-46c0-8db4-5323bd97f028","_cell_guid":"5e25553d-0d87-4b64-a9ec-2986e304e06c","trusted":true}},{"cell_type":"code","source":"pip install tensorflow-io","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.applications import VGG16","metadata":{"_uuid":"1a7537fe-742f-4bee-8e77-b6a4095326a2","_cell_guid":"192c5456-c484-49a6-a5ce-5a816be29d98","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retrieving the list of all class names from the custom dataset uploaded on kaggle","metadata":{"_uuid":"f6775c06-feef-44a6-a8ec-8ba093464bdb","_cell_guid":"7f39cf52-b7f5-4b69-ab3d-e87956074897","trusted":true}},{"cell_type":"code","source":"classes = os.listdir('/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset')\nclasses","metadata":{"_uuid":"d0f8e904-7fbe-4bfe-80cf-56d25fa42538","_cell_guid":"bc603ed7-9106-4039-ab73-b4447b3ee020","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Randomly Selecting and Displaying Video Frames from Dataset","metadata":{"_uuid":"12269478-e3f7-4d21-9c5f-0498fa26e6db","_cell_guid":"3e5d8a53-65d1-43ea-aaa2-7a962d27df6e","trusted":true}},{"cell_type":"code","source":"#Selecting randomness\nseed_constant = 3\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)\n\nplt.figure(figsize = (20, 20))\n \n# Displaying Random Video Frames from Dataset\nfor counter, index in enumerate(range(len(classes)), 1):\n    selected_class = classes[index]\n    video_files_list = os.listdir(f'/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/{selected_class}')\n    selected_video_file = random.choice(video_files_list)\n \n    video_reader = cv2.VideoCapture(f'/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/{selected_class}/{selected_video_file}')\n    video_reader.set(1, 25)\n\n    _, bgr_frame = video_reader.read()\n    bgr_frame = cv2.resize(bgr_frame ,(224,224))\n\n    video_reader.release()\n \n    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n \n    # Writing the class name on the video frame.\n    cv2.putText(rgb_frame, selected_class, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n    \n    # Displaying the frame.\n    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')","metadata":{"_uuid":"20734dd6-f938-4541-a1b7-7d9dee530d32","_cell_guid":"7f4eca55-6791-48d2-ae7a-40e47972ffe0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Specifying Data Variables","metadata":{"_uuid":"1a3c5d24-ed28-45ba-9830-ab6f683ef852","_cell_guid":"166edf98-d58c-4a45-b522-c99add5fda66","trusted":true}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset\"\n\nimg_height = 64\nimg_width = 64\n \nseq_len = 30","metadata":{"_uuid":"6f5b937c-7f18-4149-8b5a-94588e72d384","_cell_guid":"5e98a79e-0c34-41d2-9143-0d751479e9f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing and creating dataset","metadata":{"_uuid":"b3cfbe68-23c1-4a7c-8bd7-2783772a7813","_cell_guid":"d53eebe5-b6c9-4ac5-8620-c998410a46b4","trusted":true}},{"cell_type":"code","source":"def extracting_frames(path):\n    \n    frames_list = []\n    video_reader = cv2.VideoCapture(path)\n    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    skipped_frames = max(int(frame_count/seq_len), 1)\n\n    \n    # Creating an ImageDataGenerator object to perform data augmentation\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        zoom_range=0.2,\n        horizontal_flip=True,\n         # Adjust brightness\n        brightness_range=(0.8, 1.2), \n        # Apply shear transformation\n        shear_range=10,  \n        # Apply random channel shifts\n        channel_shift_range=20,  \n        # Handling boundary pixels during transformations\n        fill_mode='reflect'  \n    )\n\n    \n    for frame_counter in range(seq_len):\n\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skipped_frames)\n        success, frame = video_reader.read() \n\n\n        if not success:\n            break\n\n        resized_frame = cv2.resize(frame, (img_height, img_width))\n        aug_frame = datagen.random_transform(resized_frame)\n        normalized_frame = aug_frame / 255  \n        frames_list.append(normalized_frame)\n        \n    video_reader.release()\n\n    return frames_list","metadata":{"_uuid":"72f60805-79a1-4609-9551-16cad704335c","_cell_guid":"b6e298ba-ea8b-494a-91e5-e142911afeaf","collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset_creation():\n\n    features = []\n    paths = []\n    labels = []\n    \n    for index_of_class, name_of_class in enumerate(classes):\n        \n        print('Extracting Data of Class:', name_of_class)\n        list_of_files = os.listdir(os.path.join(dataset_path, name_of_class))\n        \n        for file in list_of_files:\n            \n            video_file_path = os.path.join(dataset_path, name_of_class, file)\n            frames = extracting_frames(video_file_path)\n            \n            if len(frames) == seq_len:\n                features.append(frames)\n                labels.append(index_of_class)\n                paths.append(video_file_path)\n                \n    features = np.asarray(features)\n    labels = np.array(labels)  \n    \n    return features, paths, labels","metadata":{"_uuid":"7b9bc2d8-392b-46dc-8a80-525b6fd07735","_cell_guid":"6931d3c3-93d7-4b51-bbbc-6af01ffb7432","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features, paths, labels = dataset_creation()\nprint(features.shape, labels.shape)","metadata":{"_uuid":"e7f25ea4-2b13-4c8e-a774-0942a548215c","_cell_guid":"8d1959a7-0df1-4cc8-bf6f-74082cc13b67","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One Hot Encoding and Data Splitting","metadata":{"_uuid":"36c3e930-4dab-4bcd-b4e8-0258d28728f3","_cell_guid":"3dcfb451-2cde-48b1-bd12-a220b387468f","trusted":true}},{"cell_type":"code","source":"one_hot_encoded_labels = to_categorical(labels)\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)","metadata":{"_uuid":"de232798-198d-46e8-8c66-e102a211e824","_cell_guid":"653aa080-54fc-4583-99fb-4e675b0048c4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{"_uuid":"572a1cd3-304f-4635-9d87-2b6376579a9f","_cell_guid":"f39d7264-4e30-42c6-bb68-9f5605e09704","trusted":true}},{"cell_type":"code","source":"def creating_model():\n    model = Sequential()\n    \n    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n    for layer in vgg.layers:\n        layer.trainable = False\n    model.add(TimeDistributed(vgg, input_shape=(seq_len, img_height, img_width, 3)))\n    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu'), input_shape = (seq_len, img_height, img_width, 3)))\n    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n    model.add(TimeDistributed(Flatten()))\n    \n    model.add(LSTM(32))\n    \n    model.add(Dense(len(classes), activation='softmax'))\n\n    model.summary()\n    \n    return model","metadata":{"_uuid":"87209562-757e-4852-81d2-d7670b689b83","_cell_guid":"e6add397-56ab-4bd1-8e75-38ae8fa86887","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(classes)\nlstm_models = []\nnum_models = 6\n\n# Training multiple LSTM models with initialization or hyperparameters\ntraining_histories = []\nfor i in range(num_models):\n    lstm_model = creating_model()\n    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    early_stopping_callback = EarlyStopping(monitor = 'accuracy', patience = 10, mode = 'max', restore_best_weights = True)\n    training_history = lstm_model.fit(x = features_train, y = labels_train, epochs = 30, batch_size = 4 , shuffle = True, validation_split = 0.25, callbacks = [early_stopping_callback])\n    lstm_models.append(lstm_model)\n    training_histories.append(training_history)\n\n# Combining the predictions of the multiple LSTM models using majority voting\ny_pred = []\nfor i in range(len(features_test)):\n    predictions = []\n    for model in lstm_models:\n        predictions.append(np.argmax(model.predict(np.expand_dims(features_test[i],axis =0))[0]))\n    y_pred.append(max(set(predictions), key=predictions.count))\n\n\n# Converting the predicted labels to one-hot encoding\ny_pred = tf.keras.utils.to_categorical(y_pred, num_classes)","metadata":{"_uuid":"b2e6cf8f-0c88-41c1-b820-5a5c80f7be2d","_cell_guid":"a0e316db-ed8d-40d1-b83d-6db5f34e73e2","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy and Model Saving","metadata":{"_uuid":"22e8aa8f-4690-4e05-929b-f0d36bded7ab","_cell_guid":"55215fe1-57d3-4a22-a443-25ebc0caa194","trusted":true}},{"cell_type":"code","source":"accuracy = tf.keras.metrics.CategoricalAccuracy()\naccuracy.update_state(labels_test, y_pred)\nprint(f\"Ensemble model accuracy: {accuracy.result().numpy()}\")","metadata":{"_uuid":"432e4203-2442-4b97-b2a8-0427dd140bdb","_cell_guid":"eb64881d-3b22-4697-8ddf-9d8b27b88140","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"/kaggle/working/Suspicious_Human_Activity_Detection.h5\")","metadata":{"_uuid":"92f7ec4a-bf21-4f98-adc2-be03c3940a68","_cell_guid":"6cfe4d6c-d278-4d2a-9468-e71c41cf9f11","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Video Annonation","metadata":{}},{"cell_type":"code","source":"def Video_Annonation(input_path, output_path, seq_len):\n\n    vid_reader = cv2.VideoCapture(input_path)\n \n    Original_width = int(vid_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    Original_height = int(vid_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    vid_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), \n                                   vid_reader.get(cv2.CAP_PROP_FPS), (Original_width, Original_height))\n \n    queue = deque(maxlen = seq_len)\n    \n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        zoom_range=0.2,\n        horizontal_flip=True,\n         # Adjust brightness\n        brightness_range=(0.8, 1.2), \n        # Apply shear transformation\n        shear_range=10,  \n        # Apply random channel shifts\n        channel_shift_range=20,  \n        # Handling boundary pixels during transformations\n        fill_mode='reflect'  \n    )\n \n    prediction = ''\n \n    while vid_reader.isOpened():\n \n        ok, frame = vid_reader.read() \n        \n        if not ok:\n            break\n            \n        resized_frame = cv2.resize(frame, (img_height, img_width))\n        aug_frame = datagen.random_transform(resized_frame)\n        normalized_frame = aug_frame / 255  \n \n        queue.append(normalized_frame)\n \n        if len(queue) == seq_len:\n \n            predicted_label = np.argmax(model.predict(np.expand_dims(queue, axis = 0))[0])\n \n            prediction = classes[predicted_label]\n \n        cv2.putText(frame, prediction, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n \n        vid_writer.write(frame)\n        \n    vid_reader.release()\n    vid_writer.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = \"/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/Vandalism/15r.mp4\"\noutput_path = \"/kaggle/working/15r.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Video_Annonation(input_path, output_path, seq_len)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Motionless Bag Detection","metadata":{}},{"cell_type":"code","source":"pip install ultralytics","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport time\n\n# Load YOLOv5 model\ndevice = torch.device('cpu')\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)\n\n# Define the class labels to detect\nclass_labels = [24, 28, 26]\n\n# Load video\nvideo_path = '/kaggle/working/15r.mp4'\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define output video writer\noutput_path = '/kaggle/working/Final_video.mp4'\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Get class names associated with YOLOv5 model\nclass_names = model.module.names if hasattr(model, 'module') else model.names\n\n# Variables for bag movement detection\nthreshold = 1000  # Threshold value for bag movement\nduration_threshold = 2  # Duration threshold in seconds\nbag_position = None  # Position of the bag (center of bounding box)\nduration = 0  # Duration of the bag within the threshold\nlast_detection_time = 0  # Time of the last detection\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n\n    if not ret:\n        break\n\n    # Perform inference on the frame\n    results = model(frame)\n\n    # Get bounding box coordinates and labels\n    boxes = results.pred[0].detach().cpu().numpy()[:, :4]\n    labels = results.pred[0].detach().cpu().numpy()[:, -1]\n\n    # Filter the bounding boxes and labels based on class labels\n    filtered_boxes = []\n    filtered_labels = []\n    for box, label in zip(boxes, labels):\n        if label in class_labels:\n            filtered_boxes.append(box)\n            filtered_labels.append(class_names[int(label)])  # Get class name based on label\n\n    # Draw bounding boxes and labels on the frame\n    for box, label in zip(filtered_boxes, filtered_labels):\n        x1, y1, x2, y2 = box.astype(int)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        # cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Check if bag is within threshold\n    if len(filtered_boxes) > 0:\n        x1, y1, x2, y2 = filtered_boxes[0].astype(int)\n        bag_x = (x1 + x2) // 2  # Calculate bag's x-coordinate\n        bag_y = (y1 + y2) // 2  # Calculate bag's y-coordinate\n\n        if bag_position is None:\n            bag_position = (bag_x, bag_y)\n            last_detection_time = time.time()\n        else:\n            # Calculate Euclidean distance between current and previous position\n            distance = np.sqrt((bag_x - bag_position[0]) ** 2 + (bag_y - bag_position[1]) ** 2)\n\n            if distance <= threshold:\n                duration += 1 / fps  # Increase duration by frame duration\n\n                if duration >= duration_threshold:\n                    # Bounding box color becomes red\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                    cv2.putText(frame, str(int(duration)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (0, 0, 255), 2)\n            else:\n                current_time = time.time()\n                elapsed_time = current_time - last_detection_time\n\n                if elapsed_time >= 1:\n                    # Bounding box color becomes green, restart duration\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                    cv2.putText(frame, str(int(duration)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (0, 255, 0), 2)\n                    bag_position = (bag_x, bag_y)\n                    duration = 0\n                    last_detection_time = current_time\n\n    # Write annotated frame to output video\n    out.write(frame)\n\n# Release video capture and writer\ncap.release()\nout.release()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}