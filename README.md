# Suspicious Activity Prediction based on Deep Learning

## Introduction
Around 3 million people worldwide have been victims of some crime daily. In January 2020, Surfshark published in its data report that the increased number of cameras does not correlate with the global crime index. This is because, after a point, it is not possible to have enough security personnel keeping track of a multitude of cameras 24 x 7 for the cameras actually to have an impact. This project proposes a deep learning-based approach for detecting suspicious activities live from CCTV footage. This project proposes an ensemble model based on Long-term Recurrent Convolutional Networks (LRCN) for the effective detection of suspicious activities and motionless objects in video data. The proposed ensemble model is trained on a large-scale dataset of labeled videos containing both normal and suspicious activities. 


## DataSet Description
We collected data from multiple datasets, namely the DCSASS dataset, Real Life Violence Situations Dataset, and UCF Crime Dataset. To create a comprehensive dataset for our specific project, we merged these datasets, resulting in over 800 videos that are categorized into predefined classes. These classes encompass a range of activities, including suspicious behaviors like fighting and vandalism, as well as non-suspicious activities such as walking and running. 
To detect objects like bags, handbags, and suitcases within the videos, we employed the 'YOLOv5' pre-trained model. This model has been widely recognized for its effectiveness in object detection tasks, making it a suitable choice for our project. By leveraging the capabilities of 'YOLOv5,' we aim to accurately identify and track these specific objects of interest within the video footage.

## DataSet Pre-Processing
We used the OpenCV library to read and extract frames from video files. The frames are extracted from all the videos in the selected classes in the dataset. Data augmentation methods were applied, including rotation, flipping, and brightness modifications. This improves the model's capacity to deal with realistic circumstances and learn invariant properties. After that, the retrieved frames are saved in a list. The data was then normalized by dividing each pixel value by 255, which is known as "255 normalizations" or "dividing by 255." to normalize pixel values to the range [0, 1] by dividing with the maximum value, i.e. 255. This ensures that the pixel values are represented as floating-point numbers between 0 and 1, which can be more convenient for specific operations or algorithms. Next, we set the height and width of each image frame to 64 and selected 30 frames per video as the sequence length. The list of classes used for training the model is specified in the \emph{classes} variable. The extracted frames, class indexes, and video file paths are then utilized for training a deep-learning model, enabling it to classify videos into specified classes. 

The dataset was then split into train and test datasets using a 75:25 split ratio. This division allows for evaluating the performance of the trained model on unseen data and estimating its generalization capabilities.

Data pre-processing processes for motionless object recognition include determining the width and height of video frames and the frames per second (fps) value. These parameters provide crucial information about the video dimensions and timings. Additionally, the \textit{threshold} and \textit{duration\_threshold} parameters are defined to determine the sensitivity of the motionless object detection algorithm.
